{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import sys\n",
    "import tqdm\n",
    "import csv\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_load_and_batch(img_list):\n",
    "    im = []\n",
    "    \n",
    "    for img_name in img_list:\n",
    "        \n",
    "        img =cv2.imread(img_name)\n",
    "        b, g, r = cv2.split(img)\n",
    "        im.append(cv2.merge([r,g,b]))\n",
    "\n",
    "            \n",
    "    return im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_make_batch(label_list):\n",
    "    output = []\n",
    "    \n",
    "    for label in label_list:\n",
    "        x = label[0]\n",
    "        y = label[1]\n",
    "        if x == None or y == None:\n",
    "            output.append(np.zeros((268,480,2)))\n",
    "            \n",
    "        else:\n",
    "            x = x//4\n",
    "            y = y//4\n",
    "            ball_map = np.zeros((268,480,2))\n",
    "            ball_map[x-2:x+2][y-2:y+2] = 1\n",
    "            output.append(ball_map)\n",
    "    \n",
    "    return output\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batch(all_data):\n",
    "    \n",
    "    img_list = []\n",
    "    label_list = []\n",
    "    \n",
    "    keys = list(all_data.keys())\n",
    "    random.shuffle(keys)\n",
    "    for key in keys:\n",
    "        img_list.append(key)\n",
    "        label_list.append(all_data[key])\n",
    "    \n",
    "    train_img = img_list[:8000]\n",
    "    test_img = img_list[8000:]\n",
    "    \n",
    "    train_label = label_list[:8000]\n",
    "    test_label = label_list[8000:]\n",
    "    \n",
    "    return train_img, test_img, train_label, test_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Conv_block(data, train=True, filter1=3, filter2=3, maxpool=True, std =1, in_ch=3, out_ch=3):    \n",
    "    \n",
    "    \n",
    "    conv1 = tf.layers.conv2d(data, filters=out_ch,kernel_size=filter1, strides=std, padding=\"SAME\", trainable=train)\n",
    "    output = tf.layers.conv2d(conv1, filters=out_ch,kernel_size=filter2, strides=1, padding='SAME',trainable=train)\n",
    "    if maxpool:\n",
    "        output = tf.layers.max_pooling2d(inputs = output, pool_size=2, strides=2, padding='VALID')\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Upsample_block(data, output_size, kernel=2, std=2):\n",
    "    \n",
    "    output = tf.layers.conv2d_transpose(data, filters=output_size, kernel_size=kernel, strides=std, padding=\"VALID\")\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Model(data, label, train, batch_size):\n",
    "\n",
    "    \n",
    "    \n",
    "    with tf.variable_scope(\"conv1\"):\n",
    "        Conv1_feature = Conv_block(data, train, filter1=7, filter2=3, maxpool=True, std=2,in_ch=3,out_ch=8)\n",
    "        Conv1_feature = tf.slice(Conv1_feature,[0,0,0,0],[batch_size,268,480,8])\n",
    "\n",
    "    with tf.variable_scope(\"conv2\"):\n",
    "        Conv2_feature = Conv_block(Conv1_feature, train, filter1=7, filter2=3, maxpool=True, in_ch=8,out_ch=16)\n",
    "        \n",
    "    with tf.variable_scope(\"conv3\"):\n",
    "        Conv3_feature = Conv_block(Conv2_feature, train, filter1=7, filter2=3, maxpool=True, in_ch=16,out_ch=32)\n",
    "        \n",
    "        \n",
    "     \n",
    "    \n",
    "    upsize = Conv1_feature.get_shape().as_list()[-1]      \n",
    "    with tf.variable_scope(\"Upsize1\"):\n",
    "        Conv2_up = Upsample_block(Conv2_feature, upsize, kernel=2, std=2)        \n",
    "    with tf.variable_scope(\"Upsize2\"):\n",
    "        upsize2 = Conv2_feature.get_shape().as_list()[-1]\n",
    "        Conv3_up = Upsample_block(Conv3_feature, upsize, kernel=4, std=4)\n",
    "        '''bias = tf.zeros([batch_size, 2, 480, 8])\n",
    "        Conv3_up = tf.concat([Conv3_up, bias], axis=1)'''\n",
    "\n",
    "        \n",
    "        \n",
    "    with tf.variable_scope(\"Concat_conv\"):\n",
    "        Conv_concat = tf.concat([Conv1_feature, Conv2_up, Conv3_up],axis=3)   \n",
    "    with tf.variable_scope(\"conv4\"):\n",
    "        Conv4_feature = Conv_block(Conv_concat, train, filter1=7, filter2=3, maxpool=False, in_ch = 56,out_ch=2)\n",
    "        #print(Conv4_feature)\n",
    "\n",
    "\n",
    "    return Conv4_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimizer(data, label, lr=0.0001):\n",
    "    #print(data)\n",
    "    logits = tf.nn.softmax(data)\n",
    "    #print(logits)\n",
    "    cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=data, labels=label)\n",
    "    loss_op = tf.reduce_mean(cross_entropy, name=\"Loss\")\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate=lr).minimize(loss_op, name=\"Train_op\")\n",
    "    \n",
    "    return logits, loss_op, train_op\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Data Load Part\n",
    "# Data Annotate In Deep_Ball_Data/Annote/ and image Deep_Ball_Data/img\n",
    "\n",
    "\n",
    "#Annotate Data Load\n",
    "f = open(\"./Deep_Ball_Data/Annote/cam.csv\", 'r', encoding='utf-8')\n",
    "k = 4\n",
    "rdr = csv.reader(f)\n",
    "Data = []\n",
    "for line in rdr:\n",
    "    if line[0][0] == 'F':continue\n",
    "    if line[1] == ' -': \n",
    "        Data.append([None, None])\n",
    "    else:\n",
    "        x = int(line[1])\n",
    "        y = int(line[2])\n",
    "        Data.append([x,y])\n",
    "                    \n",
    "f.close()\n",
    "\n",
    "#Image Data Load\n",
    "\n",
    "path = \"./Deep_Ball_Data/img/graz-arbesbach_5\"\n",
    "img_list = [os.path.join(path , x) for x in os.listdir(path)]\n",
    "\n",
    "\n",
    "#merge all data Want to shuffle\n",
    "all_data = {}\n",
    "for i,img in enumerate(img_list):\n",
    "    \n",
    "    all_data[img]= Data[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-10-001effc6a1d1>:5: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "Training.. Start..\n"
     ]
    }
   ],
   "source": [
    "n_epoch = 10\n",
    "batch = 0\n",
    "batch_size = 16\n",
    "lr = 0.001\n",
    "total_loss = 0\n",
    "train=True\n",
    "\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "\n",
    "input_image = tf.placeholder(tf.float32,[None, 1080, 1920, 3])\n",
    "label = tf.placeholder(tf.float32,[None, 268, 480, 2])\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "\n",
    "with tf.Session(config=tf.ConfigProto(gpu_options=(tf.GPUOptions(per_process_gpu_memory_fraction=0.7)), log_device_placement=True)) as sess:\n",
    "    \n",
    "    \n",
    "    output = Model(input_image, label, train, batch_size)\n",
    "    \n",
    "    logits, loss, train_op = optimizer(output, label, lr)\n",
    "    \n",
    "    \n",
    "    saver = tf.train.Saver(max_to_keep = 8000000)\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    \n",
    "    print(\"Training.. Start..\")\n",
    "    \n",
    "    for epoch in range(n_epoch):\n",
    "        counter = 0\n",
    "        batch = 0\n",
    "        total_loss = 0\n",
    "        total_acc = 0\n",
    "        Train_image, Test_image, Train_label, Test_label = make_batch(all_data)\n",
    "\n",
    "        while True:\n",
    "            counter += 1\n",
    "            train=True\n",
    "\n",
    "\n",
    "            now_img = image_load_and_batch(Train_image[batch:batch +batch_size])\n",
    "            now_label = label_make_batch(Train_label[batch:batch +batch_size])\n",
    "\n",
    "            now_total_pixcel = np.shape(now_label)[0] *np.shape(now_label)[1] *np.shape(now_label)[2]* np.shape(now_label)[3]\n",
    "           \n",
    "            logit, losses, _ = sess.run([logits, loss, train_op], feed_dict={input_image:now_img, label:now_label})\n",
    "            \n",
    "            \n",
    "            #print(type(len(np.nonzero(logit - np.array(now_label)))))\n",
    "            \n",
    "            now_acc=0\n",
    "            now_acc= (float(now_total_pixcel) - float(len(np.nonzero(logit - np.array(now_label)[0])))/float(now_total_pixcel)) *100.0\n",
    "            total_loss += losses\n",
    "            total_acc += now_acc\n",
    "\n",
    "            batch = batch+batch_size\n",
    "           \n",
    "                \n",
    "            if batch>3999 or batch>7999:\n",
    "                \n",
    "                print(\"Now...\",str(epoch),\"... \",str(batch) ,\"..End..\")\n",
    "                print(\"Now Loss..\", str(total_loss), \" Now Accuracy..\", str(total_acc / counter))\n",
    "                print(\"Validation Test Start..\")\n",
    "                \n",
    "                \n",
    "                test_batch = 0\n",
    "                total_loss = 0\n",
    "                total_acc = 0\n",
    "                train = False\n",
    "                \n",
    "                while True:\n",
    "                    \n",
    "                    test_img = image_load_and_batch(Test_image[test_batch:test_batch+ batch_size])\n",
    "                    test_label = label_make_batch(Test_label[test_batch:test_batch+ batch_size])\n",
    "                    \n",
    "                    \n",
    "                    logit, losses, train_op = sess.run([logits, loss, train_op], feed_dict={input_image:test_img, label:test_label})\n",
    "                    \n",
    "                    now_acc=0\n",
    "                    #now_acc= (now_total_pixcel - (len(np.nonzero(logit - now_label))[0])/now_total_pixcel) *100  \n",
    "                    total_acc += now_acc\n",
    "                    total_loss += losses\n",
    "                    test_batch += batch_size\n",
    "                    \n",
    "                    if test_batch>1999:\n",
    "                        \n",
    "                        print(\"Validation Test End..\")\n",
    "                        print(\"Now Loss..\", str(total_loss), \"Now Accuracy..\", str(total_acc / counter))\n",
    "                        saver.save(sess, './Deep_Ball_Result/Deep_Ball_Model_' + str(epoch)+'_'+str(batch) + '.ckpt')\n",
    "                        break\n",
    "                    \n",
    "                    \n",
    "                \n",
    "                \n",
    "                \n",
    "            if batch>7999: break\n",
    "                \n",
    "            \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
